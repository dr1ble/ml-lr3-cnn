# Лабораторная работа: Генерация подписей к изображениям (RNN/LSTM)

Этот проект реализует модель для генерации подписей к изображениям с использованием архитектуры CNN+RNN на датасете COCO Captions.

## Описание проекта

Данная работа посвящена реализации системы автоматической генерации описаний изображений с использованием:
- Предварительно обученных CNN признаков (VGG-16 fc7 с PCA сжатием)
- RNN/LSTM архитектур для генерации последовательностей текста
- Датасета COCO для обучения и валидации
- Метрик BLEU для оценки качества генерируемых подписей

## Архитектура модели

### Реализованные методы интеграции признаков изображения

Были реализованы и сравнены две архитектуры для объединения признаков изображения и текстовых данных, описанные в статье "Where to put the Image in an Image Caption Generator":

1. **Init-inject** - Вектор признаков изображения используется для инициализации начального скрытого состояния LSTM-декодера
2. **Pre-inject** - Вектор признаков изображения подается как самый первый элемент последовательности на вход LSTM

### Компоненты модели
- Embedding слой для преобразования токенов в векторы
- LSTM декодер для генерации последовательностей
- Dropout для регуляризации
- Линейный выходной слой для предсказания следующего токена

## Структура проекта

```
ml-lr3/
├── datasets/
│   └── coco_captioning/          # Данные COCO
│       ├── coco2014_captions.h5  # Подписи к изображениям
│       ├── coco2014_vocab.json   # Словарь
│       ├── *_vgg16_fc7.h5       # VGG-16 признаки
│       └── *_vgg16_fc7_pca.h5   # PCA сжатые признаки
├── notebooks/
│   ├── main.ipynb               # Основной notebook с кодом и анализом
│   ├── best_init_inject.pth     # Лучшая модель Init-inject
│   ├── best_pre_inject.pth      # Лучшая модель Pre-inject
│   └── best_exp_*.pth           # Результаты экспериментов
├── coco_utils.py                # Вспомогательный скрипт для загрузки данных
├── requirements.txt             # Список необходимых библиотек
└── README.md                    # Этот файл
```

## Настройка и запуск

### 1. Загрузка данных
**Внимание:** Датасет не включен в репозиторий из-за большого размера.

1. Скачайте архив с данными (~1 Гб) по ссылке: http://labcolor.space/coco_captioning.zip
2. Создайте в корне проекта папку `datasets`
3. Внутри нее создайте папку `coco_captioning`
4. Распакуйте содержимое скачанного архива в папку `datasets/coco_captioning/`

### 2. Создание виртуального окружения
```bash
python -m venv .venv
.venv\Scripts\activate  # Windows
# или
source .venv/bin/activate  # Linux/Mac
```

### 3. Установка зависимостей
Убедитесь, что у вас установлен Python 3.9+ и Git. Откройте терминал в корневой папке проекта и выполните:
```bash
pip install -r requirements.txt
```


### 5. Запуск Jupyter Notebook
```bash
jupyter notebook
```

Откройте `notebooks/main.ipynb` и выполните ячейки последовательно.

## Основные функции

### Предварительная обработка данных
- `COCODataset` - класс для загрузки данных
- `collate_fn` - функция для паддинга батчей
- Визуализация примеров изображений и подписей

### Модель
- `ImageCaptioningModel` - основная архитектура
- Поддержка двух методов интеграции признаков
- Маскированная cross-entropy loss

### Обучение
- `train_model` - функция обучения с early stopping
- Gradient clipping для стабильности
- ReduceLROnPlateau scheduler

### Оценка
- `calculate_bleu_scores` - вычисление BLEU метрик
- `generate_caption` - генерация подписей для новых изображений
- `visualize_predictions` - визуализация результатов

## Процесс обучения

Обе модели обучались в одинаковых условиях для честного сравнения: 7 эпохи на полном обучающем датасете COCO с использованием GPU. В процессе обучения отслеживалась функция потерь (Cross-Entropy Loss) на обучающей и валидационной выборках. Было экспериментально установлено, что обучение более 4-х эпох приводит к переобучению (росту Val Loss).

## Результаты экспериментов

### Сравнение моделей и результаты
Сравнение проводилось как визуально, так и с помощью стандартной метрики BLEU.

**Визуальная оценка:** Модель Init-inject генерировала более длинные, осмысленные и релевантные изображениям подписи. Модель Pre-inject часто генерировала лишь 2-3 слова и обрывалась на токенах `<UNK>`.

**Оценка BLEU:** Численные результаты полностью подтвердили визуальные наблюдения. Модель Init-inject показала значительно лучшие результаты по всем метрикам BLEU, превосходя вторую модель в 2-3 раза.

| Метрика | Init-inject | Pre-inject |
|---------|-------------|------------|
| BLEU-1  | **0.2927**  | 0.1453     |
| BLEU-2  | **0.1464**  | 0.0398     |
| BLEU-3  | **0.0881**  | 0.0243     |
| BLEU-4  | **0.0619**  | 0.0209     |

### Эксперименты с гиперпараметрами
Проведены эксперименты с различными гиперпараметрами:
- Скорость обучения: 1e-5, 5e-5, 1e-4, 5e-4, 1e-3
- Размер батча: 16, 32, 64, 128
- Оптимизаторы: Adam, SGD, RMSProp
- Планировщики скорости обучения: ReduceLROnPlateau, StepLR

## Специальные токены

- `<START>` (индекс 1) - начало подписи
- `<END>` (индекс 2) - конец подписи
- `<PAD>` (индекс 0) - паддинг для выравнивания длины
- `<UNK>` (индекс 3) - неизвестные слова

## Выводы по лабораторной работе

В ходе данной лабораторной работы была реализована модель для генерации подписей к изображениям (Image Captioning) на основе архитектуры CNN+RNN.

### Итоговое заключение
Экспериментально установлено, что для данной задачи архитектура **Init-inject** является значительно более эффективной. Она быстрее сходится (показывает меньший Loss) и генерирует более качественные и полные подписи, что подтверждается как визуальной оценкой, так и метрикой BLEU.
