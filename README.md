# Лабораторная работа: Генерация подписей к изображениям (RNN/LSTM)

Этот проект реализует систему автоматической генерации описаний (подписей) к изображениям с использованием гибридной нейросетевой архитектуры CNN + RNN на базе датасета MS COCO.

## Описание проекта

В рамках работы была разработана модель, способная анализировать визуальное содержимое изображения и генерировать соответствующее текстовое описание на английском языке.

**Используемые технологии:**
- **Кодировщик (Encoder):** Сверточная сеть VGG-16 (слой fc7) с применением PCA для снижения размерности признаков.
- **Декодер (Decoder):** Рекуррентная нейронная сеть на базе LSTM (Long Short-Term Memory).
- **Фреймворк:** PyTorch.
- **Метрики оценки:** BLEU-1, BLEU-2, BLEU-3, BLEU-4.

## Архитектура моделей

Были реализованы и сравнены два метода интеграции визуальных признаков в языковую модель, описанные в исследовательской литературе:

1.  **Init-inject:** Вектор признаков изображения используется однократно для инициализации начального скрытого состояния (h0, c0) LSTM-декодера.
2.  **Pre-inject:** Вектор признаков изображения конкатенируется с векторным представлением (эмбеддингом) текущего слова и подается на вход LSTM на каждом временном шаге.

## Структура проекта

```text
ml-lr3/
├── datasets/
│   └── coco_captioning/          # Данные COCO (не включены в репозиторий)
│       ├── coco2014_captions.h5
│       ├── coco2014_vocab.json
│       ├── *_vgg16_fc7.h5        # Признаки VGG16
│       └── *_vgg16_fc7_pca.h5    # Сжатые признаки
├── models/                       # Сохраненные веса обученных моделей
│   ├── best_init_inject.pth
│   └── best_pre_inject.pth
├── notebooks/
│   └── main.ipynb                # Основной файл с кодом, обучением и анализом
├── coco_utils.py                 # Модуль для загрузки и предобработки данных
├── requirements.txt              # Список зависимостей проекта
└── README.md                     # Документация
```

## Настройка и запуск

### 1. Загрузка данных
Архив с данными занимает около 1 Гб.
1. Скачайте архив по ссылке: http://labcolor.space/coco_captioning.zip
2. Создайте в корне проекта директорию `datasets/coco_captioning/`.
3. Распакуйте содержимое архива в созданную директорию.

### 2. Установка окружения
Для работы проекта требуется Python 3.9 или выше. Рекомендуется использование виртуального окружения.
```bash
pip install -r requirements.txt
```

### 3. Запуск
Откройте основной файл в Jupyter Notebook:
```bash
jupyter notebook notebooks/main.ipynb
```

## Параметры обучения

Для достижения оптимального качества генерации и устранения эффекта переобучения были подобраны следующие гиперпараметры:

*   **Датасет:** Использован полный набор данных MS COCO (>80 000 изображений).
*   **Размер батча (Batch Size):** 256. Большой размер батча обеспечил более стабильную оценку градиента и ускорил сходимость модели.
*   **Оптимизация:** Алгоритм Adam (learning rate = 1e-3) с планировщиком ReduceLROnPlateau.
*   **Регуляризация:** Dropout (0.3) и Gradient Clipping (max_norm=5.0).
*   **Остановка обучения:** Early Stopping при отсутствии улучшений Validation Loss.

В результате обучения удалось снизить ошибку валидации (**Validation Loss**) ниже целевого порога **2.5**.

## Результаты экспериментов

### 1. Численные метрики
Оценка проводилась на отложенной выборке из 500 изображений. Сравнение показало преимущество архитектуры Pre-inject.

| Метрика | Init-inject | Pre-inject | Лучший результат |
|:-------:|:-----------:|:----------:|:----------------:|
| **BLEU-1** | 0.2584 | **0.2800** | **Pre-inject** |
| **BLEU-2** | 0.1297 | **0.1462** | **Pre-inject** |
| **BLEU-3** | 0.0747 | **0.0892** | **Pre-inject** |
| **BLEU-4** | 0.0499 | **0.0599** | **Pre-inject** |

### 2. Функция потерь (Loss)
Обе модели продемонстрировали стабильную сходимость без признаков переобучения:
- **Init-inject Val Loss:** ~2.35
- **Pre-inject Val Loss:** ~2.37

### 3. Качественный анализ
При анализе результатов на отдельных изображениях качество генерации часто превышает средние показатели. Для изображений с четко различимыми объектами метрика **BLEU-1 достигает значений 0.50–0.70**. Модель Pre-inject демонстрирует лучшую способность удерживать контекст и формировать более сложные грамматические конструкции по сравнению с Init-inject.

## Заключение

В ходе выполнения лабораторной работы была успешно реализована система генерации подписей к изображениям. Были выполнены все поставленные задачи: от предварительной обработки данных и реализации нейросетевых архитектур до проведения сравнительного анализа и оценки качества.

**Ключевые выводы:**
1.  **Роль объема данных:** Переход от ограниченной выборки к полному датасету стал решающим фактором для улучшения обобщающей способности модели и снижения ошибки валидации до уровня 2.35.
2.  **Эффективность архитектур:** Экспериментально подтверждено, что архитектура **Pre-inject** является более эффективной для данной задачи, обеспечивая прирост метрик BLEU на 8-20% по сравнению с Init-inject.
3.  **Стабилизация обучения:** Увеличение размера батча до 256 позволило существенно сгладить процесс обучения и достичь более глубокого минимума функции потерь.

На основе проведенных экспериментов, оптимальной конфигурацией для задачи Image Captioning в рамках данного проекта является архитектура **Pre-inject**, обученная на полном наборе данных с использованием GPU.
